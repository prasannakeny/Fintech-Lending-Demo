{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the loan application id here (100211, 200211 or 300211)\n",
    "\n",
    "application_id='300211'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from prettytable import PrettyTable, ALL\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = f\"\"\"SELECT * FROM `<<project name>>.<<dataset name>>.loan-application` where application_id='{application_id}'\"\"\"\n",
    "job = client.query(query)\n",
    "\n",
    "df_applicant = job.to_dataframe()\n",
    "df_applicant2 = df_applicant.set_index(\"application_id\", drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = PrettyTable(['Application Field', 'Value'])\n",
    "t.align[\"Application Field\"] = \"l\"\n",
    "t.align[\"Value\"] = \"l\"\n",
    "t.hrules=ALL\n",
    "\n",
    "# Applicant Data\n",
    "applicant_name = df_applicant2.loc[application_id,\"applicant_name\"]\n",
    "applicant_employer = df_applicant2.loc[application_id,\"applicant_employer\"]\n",
    "applicant_dob_1 = df_applicant2.loc[application_id,\"applicant_dob\"]\n",
    "applicant_dob = applicant_dob_1.strftime(\"%d-%m-%Y\")\n",
    "gender = df_applicant2.loc[application_id,\"gender\"]\n",
    "residential_area_type = df_applicant2.loc[application_id,\"residential_area_type\"]\n",
    "loan_amount = df_applicant2.loc[application_id,\"loan_amount\"]\n",
    "loan_term = df_applicant2.loc[application_id,\"loan_term\"]\n",
    "applicant_income = df_applicant2.loc[application_id,\"applicant_income\"] \n",
    "coapplicant_income = df_applicant2.loc[application_id,\"coapplicant_income\"]\n",
    "employment_type = df_applicant2.loc[application_id,\"employment_type\"]\n",
    "education_status = df_applicant2.loc[application_id,\"education_status\"]\n",
    "marital_status = df_applicant2.loc[application_id,\"marital_status\"]\n",
    "dependents = df_applicant2.loc[application_id,\"dependents\"] \n",
    "credit_history = df_applicant2.loc[application_id,\"credit_history\"]\n",
    "\n",
    "\n",
    "t.add_row([\"Applicant Name:\", applicant_name])\n",
    "t.add_row([\"Applicant Employer:\", applicant_employer])\n",
    "t.add_row([\"Applicant DoB:\", applicant_dob])\n",
    "t.add_row([\"Gender:\", gender])\n",
    "t.add_row([\"Residential Area:\", residential_area_type])\n",
    "t.add_row([\"Loan Amount:\", loan_amount])\n",
    "t.add_row([\"Loan Term:\", loan_term])\n",
    "t.add_row([\"Applicant Income:\", applicant_income])\n",
    "t.add_row([\"Coapplicant Income:\", coapplicant_income])\n",
    "t.add_row([\"Employment Type:\", employment_type])\n",
    "t.add_row([\"Education Status:\", education_status])\n",
    "t.add_row([\"Marital Status:\", marital_status])\n",
    "t.add_row([\"Dependents:\", dependents])\n",
    "t.add_row([\"Credit History:\", credit_history])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id= << GCP project id >>\n",
    "location = 'us'\n",
    "payslip_emp_name = ''\n",
    "payslip_employer_name = ''\n",
    "dl_full_name = ''\n",
    "dl_date_of_birth = ''\n",
    "dl_dob_match = ''\n",
    "doc_ver_result = ''\n",
    "\n",
    "#pip install thefuzz, pip install python-Levenshtein, #pip install fuzzywuzzy\n",
    "\n",
    "processor_id = << Doc AI processor id >> # PAYSLIP PARSER\n",
    "file_path = f\"./LoanApplications/{application_id}/payslip.pdf\"\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from prettytable import PrettyTable, ALL\n",
    "from PIL import Image, ImageDraw\n",
    "#from thefuzz import fuzz\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def process_document(\n",
    "    project_id=project_id, location=location, processor_id=processor_id,  file_path=file_path\n",
    "):\n",
    "    \n",
    "    # Instantiates a client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    # PAYSLIP\n",
    "    #############################################################\n",
    "    \n",
    "    project_id= << GCP project id >>\n",
    "    processor_id = << Doc AI processor id >> # PAYSLIP PARSER\n",
    "    file_path = f\"./LoanApplications/{application_id}/payslip.pdf\"\n",
    "\n",
    "    # The full resource name of the processor\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Read the file into memory\n",
    "    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n",
    "\n",
    "    # Configure the process request\n",
    "    request = {\"name\": name, \"document\": document}\n",
    "\n",
    "    # Use the Document AI client to process the sample form\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    document = result.document\n",
    "    document_text = document.text\n",
    "    \n",
    "    #############################################################\n",
    "    # Entity extraction for PAYSLIP\n",
    "    #############################################################\n",
    "    \n",
    "    entities = document.entities\n",
    "    \n",
    "    # Grab each key/value pair and their corresponding confidence scores.\n",
    "    t = PrettyTable(['Type', 'Value', 'Confidence'])\n",
    "    t.hrules=ALL\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_type = entity.type_\n",
    "        value = entity.mention_text\n",
    "        confience = round(entity.confidence,4)\n",
    "        if (entity_type == 'employee_name'): payslip_emp_name = value\n",
    "        if (entity_type == 'employer_name'): payslip_employer_name = value\n",
    "        t.add_row([entity_type, value.strip()[:100], confience])\n",
    "        \n",
    "    print(\"\\n\\nPayslip Entities.\")\n",
    "    print(t)\n",
    "    \n",
    "    #############################################################\n",
    "    # DRIVER'S LICENSE\n",
    "    #############################################################\n",
    "    \n",
    "    project_id= << GCP project id >>\n",
    "    processor_id = << Doc AI processor id >> # DL PARSER\n",
    "    file_path = f\"./LoanApplications/{application_id}/drivinglicense.pdf\"\n",
    "\n",
    "    # The full resource name of the processor\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Read the file into memory\n",
    "    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n",
    "\n",
    "    # Configure the process request\n",
    "    request = {\"name\": name, \"document\": document}\n",
    "\n",
    "    # Use the Document AI client to process the sample form\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    document = result.document\n",
    "    document_text = document.text\n",
    "    \n",
    "    #############################################################\n",
    "    # Entity extraction for DL\n",
    "    #############################################################\n",
    "    \n",
    "    entities = document.entities\n",
    "    \n",
    "    # Grab each key/value pair and their corresponding confidence scores.\n",
    "    t = PrettyTable(['Type', 'Value', 'Confidence'])\n",
    "    t.hrules=ALL\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_type = entity.type_\n",
    "        value = entity.mention_text\n",
    "        confience = round(entity.confidence,4)\n",
    "        if (entity_type == 'full_name'): dl_full_name = value\n",
    "        if (entity_type == 'date_of_birth'): dl_date_of_birth = value\n",
    "        t.add_row([entity_type, value.strip()[:100], confience])\n",
    "        \n",
    "    print(\"\\n\\nIndia DL Entities.\")\n",
    "    print(t)\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    t = PrettyTable(['Entity','Loan Appl', 'Payslip', 'PS Similarity', 'Driver Lic', 'DL Similarity','Result'])\n",
    "    \n",
    "    t.align[\"Entity\"] = \"l\"\n",
    "    t.align[\"Loan Appl\"] = \"l\"\n",
    "    t.align[\"Payslip\"] = \"l\"\n",
    "    t.align[\"PS Similarity\"] = \"c\"\n",
    "    t.align[\"Driver Lic\"] = \"l\"\n",
    "    t.align[\"DL Similarity\"] = \"c\"\n",
    "    t.hrules=ALL\n",
    "    \n",
    "    if (fuzz.ratio(applicant_name.upper(), payslip_emp_name.upper()) > 70 and fuzz.ratio(applicant_name, dl_full_name) > 70):\n",
    "        doc_ver_result = 'PASS'\n",
    "    else:\n",
    "        doc_ver_result = 'FAIL'\n",
    "    t.add_row(['Name', applicant_name, payslip_emp_name, \n",
    "               fuzz.ratio(applicant_name.upper(), payslip_emp_name.upper()),\n",
    "               dl_full_name,fuzz.ratio(applicant_name, dl_full_name),doc_ver_result])\n",
    "    \n",
    "    if (fuzz.ratio(applicant_employer.upper(), payslip_employer_name.upper()) > 70):\n",
    "        doc_ver_result = 'PASS'\n",
    "    else:\n",
    "        doc_ver_result = 'FAIL'\n",
    "    t.add_row(['Employer', applicant_employer, payslip_employer_name, \n",
    "               fuzz.ratio(applicant_employer.upper(), payslip_employer_name.upper()),\n",
    "               '--','--',doc_ver_result])\n",
    "    \n",
    "    clean_dl_dob = dl_date_of_birth.replace(\"/\",\"\").replace(\"-\",\"\")\n",
    "    clean_appl_dob = applicant_dob.replace(\"/\",\"\").replace(\"-\",\"\")\n",
    "    # print(\"Check this\", fuzz.ratio(clean_dl_dob, clean_appl_dob))\n",
    "\n",
    "    \n",
    "    if (fuzz.ratio(clean_dl_dob, clean_appl_dob) < 100): \n",
    "        dl_dob_match = 'Mismatch Alert'\n",
    "        doc_ver_result = 'FAIL'\n",
    "    else: \n",
    "        dl_dob_match = 'Matched'\n",
    "        doc_ver_result = 'PASS'\n",
    "    t.add_row(['Birth Date', applicant_dob, '--','--',dl_date_of_birth,dl_dob_match,doc_ver_result])\n",
    "\n",
    "\n",
    "    print(\"\\n\\nDocument Verification Checks.\")\n",
    "    print(t)\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    # Identity Proofing\n",
    "    #############################################################\n",
    "    \n",
    "    project_id= << GCP project id >>\n",
    "    processor_id = << Doc AI processor id >> # Identity Proofing Parser\n",
    "    file_path = f\"./LoanApplications/{application_id}/drivinglicense.pdf\"\n",
    "\n",
    "    # The full resource name of the processor\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Read the file into memory\n",
    "    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n",
    "\n",
    "    # Configure the process request\n",
    "    request = {\"name\": name, \"document\": document}\n",
    "\n",
    "    # Use the Document AI client to process the sample form\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    document = result.document\n",
    "    document_text = document.text\n",
    "    \n",
    "    #############################################################\n",
    "    # Entity extraction for DL\n",
    "    #############################################################\n",
    "    \n",
    "    entities = document.entities\n",
    "    \n",
    "    # Grab each key/value pair and their corresponding confidence scores.\n",
    "    t = PrettyTable(['Type', 'Value'])\n",
    "    t.hrules=ALL\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_type = entity.type_\n",
    "        value = entity.mention_text\n",
    "        confience = round(entity.confidence,4)\n",
    "        t.add_row([entity_type, value.strip()[:50]])\n",
    "        \n",
    "    print(\"\\n\\nID Doc Proofing Results.\")\n",
    "    print(t)\n",
    "    \n",
    "\n",
    "    \n",
    "def get_text(doc_element: dict, document: dict):\n",
    "    \"\"\"\n",
    "    Document AI identifies form fields by their offsets\n",
    "    in document text. This function converts offsets\n",
    "    to text snippets.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    for segment in doc_element.text_anchor.text_segments:\n",
    "        start_index = (\n",
    "            int(segment.start_index)\n",
    "            if segment in doc_element.text_anchor.text_segments\n",
    "            else 0\n",
    "        )\n",
    "        end_index = int(segment.end_index)\n",
    "        response += document.text[start_index:end_index]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(project_id,location,processor_id,file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alernate Data - SMS data inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_text_classification_single_label_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    instance = predict.instance.TextClassificationPredictionInstance(\n",
    "        content=content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    \n",
    "    t = PrettyTable(['Classification','Confidence'])\n",
    "    t.align[\"Classification\"] = \"l\"\n",
    "\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        #print(\" prediction:\", dict(prediction))\n",
    "    \n",
    "        for i, kv in enumerate(prediction.items()):\n",
    "            if (kv[0] == \"displayNames\"): \n",
    "                #print(\"Field Names\", kv[1])\n",
    "                fieldnames = kv[1]\n",
    "            if (kv[0] == \"confidences\"): \n",
    "                #print(\"Start Offsets\", kv[1])\n",
    "                myconfidences = kv[1]\n",
    "\n",
    "    for pk in range(len(fieldnames)):\n",
    "        myconfnew = \"{:.2f}\".format(myconfidences[pk])\n",
    "        t.add_row ([fieldnames[pk], myconfnew])\n",
    "            \n",
    "    print(\"\\n\\nClassification from this SMS.\")\n",
    "    t.sortby=\"Confidence\"\n",
    "    t.reversesort = True\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = f\"\"\"SELECT sms_text FROM `<< project id >>.<< dataset id >>.loan-appl-sms-data` where application_id={application_id} and \n",
    "classification != 'credit_txn'\"\"\"\n",
    "job = client.query(query)\n",
    "\n",
    "\n",
    "df_sms = job.to_dataframe()\n",
    "for mpk in range(len(df_sms)):\n",
    "    \n",
    "    sms_txt = df_sms.loc[mpk,\"sms_text\"]\n",
    "    print(\"\\nSMS text: \", sms_txt)\n",
    "    predict_text_classification_single_label_sample(\n",
    "        project=\"54002134587\",\n",
    "        endpoint_id=\"4676966222791704576\",\n",
    "        location=\"us-central1\",\n",
    "        content=sms_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_text_entity_extraction_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema\n",
    "    instance = predict.instance.TextExtractionPredictionInstance(\n",
    "        content=content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    #print(\"response\")\n",
    "    #print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/text_extraction_1.0.0.yaml for the format of the predictions.\n",
    "    \n",
    "    t = PrettyTable(['Entity Name','Entity Value'])\n",
    "    t.align[\"Entity Name\"] = \"l\"\n",
    "    \n",
    "    \n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        for i, kv in enumerate(prediction.items()):\n",
    "            if (kv[0] == \"displayNames\"): \n",
    "                #print(\"Field Names\", kv[1])\n",
    "                fieldnames = kv[1]\n",
    "            if (kv[0] == \"textSegmentStartOffsets\"): \n",
    "                #print(\"Start Offsets\", kv[1])\n",
    "                startoffsets = kv[1]\n",
    "            if (kv[0] == \"textSegmentEndOffsets\"): \n",
    "                #print(\"End Offsets\", kv[1])\n",
    "                endoffsets = kv[1]\n",
    "\n",
    "    for pk2 in range(len(fieldnames)):\n",
    "        t.add_row ([fieldnames[pk2], content[int(startoffsets[pk2]):int(endoffsets[pk2])]])\n",
    "            \n",
    "    print(\"\\n\\nEntities extracted from this SMS.\")\n",
    "    print(t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = f\"\"\"SELECT sms_text FROM `<< project id >>.<< dataset id >>.loan-appl-sms-data` where application_id={application_id} and \n",
    "classification = 'credit_txn'\"\"\"\n",
    "job = client.query(query)\n",
    "\n",
    "\n",
    "df_sms = job.to_dataframe()\n",
    "for mpk in range(len(df_sms)):\n",
    "    sms_txt = df_sms.loc[mpk,\"sms_text\"]\n",
    "    print(\"\\nSMS text: \", sms_txt)\n",
    "    predict_text_entity_extraction_sample(\n",
    "        project=\"54002134587\",\n",
    "        endpoint_id=\"7088696599806738432\",\n",
    "        location=\"us-central1\",\n",
    "        content=sms_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = f\"\"\"SELECT FORMAT_DATE('%b %Y', txn_date) txn_month, avg(avl_balance) monthly_average_balance \\\n",
    "FROM `fintech-demo-prasanna-386521.loans.loan-appl-sms-data` where \\\n",
    "application_id = {application_id} and classification = 'credit_txn' group by txn_month\"\"\"\n",
    "\n",
    "job = client.query(query)\n",
    "df_avl_bal = job.to_dataframe()\n",
    "print(tabulate(df_avl_bal, headers='keys', tablefmt='psql'))\n",
    "\n",
    "query = f\"\"\"SELECT avg(avl_balance) monthly_average_balance \\\n",
    "FROM `<< project id >>.<< dataset id >>.loan-appl-sms-data` where \\\n",
    "application_id = {application_id} and classification = 'credit_txn'\"\"\"\n",
    "\n",
    "job = client.query(query)\n",
    "df_avl_bal = job.to_dataframe()\n",
    "\n",
    "avg_monthly_bal = df_avl_bal.loc[0,\"monthly_average_balance\"]\n",
    "print(\"\\n\\nMonthly Average Balance: \",\"{:.2f}\".format(avg_monthly_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = << Model ID >>\n",
    "ENDPOINT_ID = << Endpoint ID >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features preparation\n",
    "\n",
    "if credit_history == 'True': \n",
    "    credit_history_0 = 0\n",
    "    credit_history_1 = 1\n",
    "else:\n",
    "    credit_history_0 = 1\n",
    "    credit_history_1 = 0\n",
    "    \n",
    "if marital_status == 'Married':\n",
    "    Married_No = 0\n",
    "    Married_Yes = 1\n",
    "else:\n",
    "    Married_No = 1\n",
    "    Married_Yes = 0\n",
    "        \n",
    "if gender == 'M':\n",
    "    gender_Female = 0\n",
    "    gender_Male = 1 \n",
    "elif gender == 'F':\n",
    "    gender_Female = 1\n",
    "    gender_Male = 0 \n",
    "    \n",
    "if education_status == 'Graduate':\n",
    "    Education_Graduate = 1\n",
    "    Education_Not_Graduate = 0\n",
    "else:\n",
    "    Education_Graduate = 0\n",
    "    Education_Not_Graduate = 1\n",
    "        \n",
    "if employment_type == 'Salaried':\n",
    "    Self_Employed_No = 1 \n",
    "    Self_Employed_Yes = 0\n",
    "elif employment_type == 'Business':\n",
    "    Self_Employed_No = 0 \n",
    "    Self_Employed_Yes = 1\n",
    "        \n",
    "if residential_area_type == 'Rural':\n",
    "    Property_Area_Rural = 1\n",
    "    Property_Area_Semiurban = 0\n",
    "    Property_Area_Urban = 0\n",
    "elif residential_area_type == 'Semiurban':\n",
    "    Property_Area_Rural = 0\n",
    "    Property_Area_Semiurban = 1\n",
    "    Property_Area_Urban = 0\n",
    "else:\n",
    "    Property_Area_Rural = 0\n",
    "    Property_Area_Semiurban = 0\n",
    "    Property_Area_Urban = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestr = \"{\\n  \\\"instances\\\": [\\n    [\" + f'{dependents}, {applicant_income},{coapplicant_income},\\\n",
    "{loan_amount},{loan_term},{avg_monthly_bal},{gender_Female},{gender_Male},{Married_No},{Married_Yes},{Education_Graduate},\\\n",
    "{Education_Not_Graduate},{Self_Employed_No},{Self_Employed_Yes},{credit_history_0},{credit_history_1},\\\n",
    "{Property_Area_Rural},{Property_Area_Semiurban},{Property_Area_Urban}'+ \"]\\n  ]\\n}\"\n",
    "\n",
    "\n",
    "text_file = open(\"predictions.json\", \"w\")\n",
    "n = text_file.write(featurestr)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai endpoints predict $ENDPOINT_ID \\\n",
    "--json-request=predictions.json \\\n",
    "--region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
